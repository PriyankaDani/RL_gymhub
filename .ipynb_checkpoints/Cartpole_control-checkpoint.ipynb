{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2cf5a9",
   "metadata": {},
   "source": [
    "## Cartpole Control\n",
    "\n",
    "### Author: Priyanka Dani\n",
    "\n",
    "- Project 1 in RL\n",
    "- Date: 23/11/2022\n",
    "- Reference: [https://www.youtube.com/watch?v=Mut_u40Sqz4&t=1154s]\n",
    "- Cartpole Documentation: [https://www.gymlibrary.dev/environments/classic_control/cart_pole/]\n",
    "- Further references: \n",
    "    - [https://spinningup.openai.com/en/latest/user/introduction.html#what-this-is]\n",
    "    - [https://stable-baselines3.readthedocs.io/en/master/guide/algos.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b64501",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the dependencies ##\n",
    "\n",
    "import os #useful in saving our model and logout fast\n",
    "import gym #environment\n",
    "from stable_baselines3 import PPO #imported one of the RL algorithms PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv #its a wrapper around the environment\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #makes it easier to evaluate the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8004e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the environment ##\n",
    "\n",
    "environment_name = \"CartPole-v1\"\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4092df34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 12.0\n",
      "Episode: 2 Score: 30.0\n",
      "Episode: 3 Score: 18.0\n",
      "Episode: 4 Score: 14.0\n",
      "Episode: 5 Score: 22.0\n"
     ]
    }
   ],
   "source": [
    "## Running the environment ##\n",
    "\n",
    "#had to install pyglet\n",
    "\n",
    "episodes = 5 #episode length is 500 frames for v1 and 200 frames for v0\n",
    "for episode in range(1, episodes + 1):\n",
    "    state = env.reset() #this gives the observations of the environment\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() #allows us to view the environment\n",
    "        #env.action_space #will give you the nature of the action space\n",
    "        action = env.action_space.sample() #generates a random action\n",
    "        #env.observation_space #will give you the nature of the action space\n",
    "        n_state, reward, done, info = env.step(action) #observations, rewards, whether or not the episode is done\n",
    "        score += reward\n",
    "    print('Episode: {} Score: {}'.format(episode, score))\n",
    "env.close() #shuts the window down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ec7b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Understanding the environment ##\n",
    "\n",
    "env.action_space #discrete(2,)\n",
    "env.observation_space #box(4,) like an ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a20bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Logs'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To save the logs of the training ##\n",
    "log_path = os.path.join('Training', 'Logs') #manually created directories\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6a49625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "## Creating the agent ## \n",
    "\n",
    "env = gym.make(environment_name) #recreate the environment\n",
    "env = DummyVecEnv([lambda: env]) #wrapped the environment in dummy\n",
    "agent = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path) #define the agent\n",
    "#MlpPolicy: Multilayer Perceptron policy, environment handle, we want to log the result, where to log\n",
    "\n",
    "#PPO?? #learn more about the policy arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a64fdb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 928  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 998         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008147578 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00338    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.66        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 54.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 994         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011604205 |\n",
      "|    clip_fraction        | 0.0921      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.086       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 31.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1010        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007283748 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 49.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1027        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007542871 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 996         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010349756 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 60.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1009        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005808973 |\n",
      "|    clip_fraction        | 0.0809      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 51.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1020        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005189318 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.59        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00931    |\n",
      "|    value_loss           | 30          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1023        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009657846 |\n",
      "|    clip_fraction        | 0.092       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.55        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00863    |\n",
      "|    value_loss           | 32.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1026        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003464701 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f9537794760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training the agent ##\n",
    "\n",
    "agent.learn(total_timesteps = 20000) #recommended 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b008054",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the agent ##\n",
    "\n",
    "PPO_path = os.path.join('Training', 'Saved_agents', 'PPO_Cartpole_Agent')\n",
    "agent.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete an agent ##\n",
    "\n",
    "#del agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46290b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a model ##\n",
    "\n",
    "#agent = PPO.load(PPO_path, env=env) #this will load and start the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65dc45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/playmate/miniconda3/envs/rl_playground/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluating the trained agent ##\n",
    "evaluate_policy(agent, env, n_eval_episodes = 10, render =True)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f51cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: [467.]\n",
      "Episode: 2 Score: [144.]\n",
      "Episode: 3 Score: [413.]\n",
      "Episode: 4 Score: [141.]\n",
      "Episode: 5 Score: [500.]\n"
     ]
    }
   ],
   "source": [
    "## Testing the agent ##\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = agent.predict(obs) #model now determines the action\n",
    "        obs, reward, done, info = env.step(action) #model's chosen action passed to env\n",
    "        score += reward\n",
    "    print('Episode: {} Score: {}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48fe8d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close() #shuts the window down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorboard viewing training logs ##\n",
    "\n",
    "#training_log_path = os.path.join('Training', 'Logs','PPO_1')\n",
    "\n",
    "#Run this in the terminal better \n",
    "#cd Documents/Training/Logs/PPO_1 --> #tensorboard --logdir=.\n",
    "\n",
    "#Run in jupyter, need to close later\n",
    "#!tensorboard --logdir=training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "537a95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALLBACK Functions ##\n",
    "\n",
    "# Especially useful while training large models. It automatically stops training and saves the model when the reward\n",
    "# threshold is achieved.\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "save_path = os.path.join('Training', 'Saved_agents')\n",
    "\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold = 475, verbose = 1)\n",
    "\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best = stop_callback, \n",
    "                             eval_freq = 10000, \n",
    "                             best_model_save_path = save_path, \n",
    "                             verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e79592ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training/Logs/PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1503 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1214        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008082055 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00692     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.71        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 54.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1184        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011559168 |\n",
      "|    clip_fraction        | 0.0922      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0535      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 35          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 985         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007799989 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 54.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=332.20 +/- 139.13\n",
      "Episode length: 332.20 +/- 139.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | 332         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007131606 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 65.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 955   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 959         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011351196 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 950         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015293736 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 46.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 949         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007244001 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.53        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00776    |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 958          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057991953 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.574       |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.57         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0041      |\n",
      "|    value_loss           | 41.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043644905 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.558       |\n",
      "|    explained_variance   | 0.174        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62.4         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00476     |\n",
      "|    value_loss           | 67.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 500.00  is above the threshold 475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(487.4, 37.8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating new agent for callback ##\n",
    "\n",
    "agent_callback = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path)\n",
    "agent_callback.learn(total_timesteps = 20000, callback  = eval_callback)\n",
    "\n",
    "evaluate_policy(agent_callback, env, n_eval_episodes = 10, render =True) #evaluate agent\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b86b798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training/Logs/PPO_2\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1349 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 781          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149679445 |\n",
      "|    clip_fraction        | 0.224        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.681       |\n",
      "|    explained_variance   | 0.000136     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.57         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0232      |\n",
      "|    value_loss           | 20           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 745         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016087193 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    value_loss           | 29.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 734         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010369407 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 47.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=342.40 +/- 100.64\n",
      "Episode length: 342.40 +/- 100.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 342         |\n",
      "|    mean_reward          | 342         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008482681 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.72        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 43.4        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 693   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 702         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006286611 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.11        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 675          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074120527 |\n",
      "|    clip_fraction        | 0.0969       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.534       |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.1         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 27.1         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 682        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01636013 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.524     |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.63       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.00956   |\n",
      "|    value_loss           | 12.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 690         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015515522 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.15        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    value_loss           | 6.85        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011249139 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.141       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00734    |\n",
      "|    value_loss           | 2.61        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 671   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Specifying a different architecture of the neural network of the policy ##\n",
    "\n",
    "net_arch = [dict(pi = [128, 128, 128, 128], vf = [128, 128, 128, 128])] \n",
    "#here we have a 4 layered 128 neuron each NN for policy and value function of PPO\n",
    "\n",
    "agent_netarch = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path, policy_kwargs = {'net_arch':net_arch})\n",
    "\n",
    "agent_netarch.learn(total_timesteps = 20000, callback = eval_callback)\n",
    "\n",
    "evaluate_policy(agent_callback, env, n_eval_episodes = 5, render =True) #evaluate agent\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dd895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
